


```python
# main.py

import feedparser
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from celery import Celery
import spacy
from datetime import datetime
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Initialize SQLAlchemy
Base = declarative_base()
engine = create_engine('postgresql://username:password@localhost/news_db')
Session = sessionmaker(bind=engine)

# Initialize Celery
celery_app = Celery('news_tasks', broker='redis://localhost:6379/0')

# Initialize spaCy
nlp = spacy.load("en_core_web_sm")

# Define the Article model
class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    title = Column(String(255), nullable=False)
    content = Column(Text, nullable=False)
    pub_date = Column(DateTime, nullable=False)
    source_url = Column(String(255), nullable=False)
    category = Column(String(50), nullable=True)

# Create tables
Base.metadata.create_all(engine)

# List of RSS feeds
RSS_FEEDS = [
    "http://rss.cnn.com/rss/cnn_topstories.rss",
    "http://qz.com/feed",
    "http://feeds.foxnews.com/foxnews/politics",
    "http://feeds.reuters.com/reuters/businessNews",
    "http://feeds.feedburner.com/NewshourWorld",
    "https://feeds.bbci.co.uk/news/world/asia/india/rss.xml"
]

def parse_feed(feed_url):
    try:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            article = {
                'title': entry.title,
                'content': entry.summary,
                'pub_date': datetime(*entry.published_parsed[:6]),
                'source_url': entry.link
            }
            process_article.delay(article)
    except Exception as e:
        logging.error(f"Error parsing feed {feed_url}: {str(e)}")

@celery_app.task
def process_article(article):
    session = Session()
    try:
        # Check for duplicates
        existing = session.query(Article).filter_by(source_url=article['source_url']).first()
        if existing:
            logging.info(f"Duplicate article found: {article['title']}")
            return

        # Categorize the article
        category = categorize_article(article['title'] + " " + article['content'])
        
        # Create new Article object
        new_article = Article(
            title=article['title'],
            content=article['content'],
            pub_date=article['pub_date'],
            source_url=article['source_url'],
            category=category
        )
        
        # Add to database
        session.add(new_article)
        session.commit()
        logging.info(f"Added new article: {article['title']}")
    except Exception as e:
        logging.error(f"Error processing article {article['title']}: {str(e)}")
        session.rollback()
    finally:
        session.close()

def categorize_article(text):
    doc = nlp(text)
    
    # Simple rule-based categorization
    terrorism_keywords = ["terrorism", "protest", "riot", "unrest"]
    positive_keywords = ["positive", "uplifting", "inspiring"]
    disaster_keywords = ["disaster", "earthquake", "flood", "hurricane"]
    
    for token in doc:
        if token.text.lower() in terrorism_keywords:
            return "Terrorism / protest / political unrest / riot"
        elif token.text.lower() in positive_keywords:
            return "Positive/Uplifting"
        elif token.text.lower() in disaster_keywords:
            return "Natural Disasters"
    
    return "Others"

if __name__ == "__main__":
    for feed in RSS_FEEDS:
        parse_feed(feed)

```

Now, let's go through the steps to implement this application:

1. Set up the environment:
   - Install Python 3.x
   - Install required libraries: `pip install feedparser sqlalchemy celery spacy psycopg2-binary redis`
   - Download spaCy English model: `python -m spacy download en_core_web_sm`
   - Install and set up PostgreSQL and Redis

2. Create the main Python script (main.py) as shown in the artifact above.

3. Configure the database:
   - Update the database URL in the script to match your PostgreSQL settings
   - Create a new database named 'news_db' in PostgreSQL

4. Set up Celery:
   - Create a file named `celery_config.py`:

```python
# celery_config.py

from celery import Celery

app = Celery('news_tasks', broker='redis://localhost:6379/0')

app.conf.update(
    result_backend='redis://localhost:6379/0',
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='UTC',
    enable_utc=True,
)

```

5. Create a script to run the feed parser periodically:

```python
# run_parser.py

from main import parse_feed, RSS_FEEDS
import schedule
import time

def job():
    for feed in RSS_FEEDS:
        parse_feed(feed)

schedule.every(1).hour.do(job)

if __name__ == "__main__":
    while True:
        schedule.run_pending()
        time.sleep(1)

```

6. Start the Celery worker:
   Open a terminal and run:
   ```
   celery -A main worker --loglevel=info
   ```

7. Run the parser script:
   Open another terminal and run:
   ```
   python run_parser.py
   ```

8. Monitor the application:
   - Check the logs for any errors or issues
   - Verify that articles are being added to the database

9. Improve the categorization:
   The current categorization is very basic. To improve it, you could:
   - Use a machine learning model trained on labeled news data
   - Implement more sophisticated rule-based classification
   - Use sentiment analysis for the "Positive/Uplifting" category

10. Add error handling and retries:
    Implement retry logic for failed tasks and network errors

11. Optimize performance:
    - Use bulk inserts for better database performance
    - Implement caching to reduce duplicate processing

12. Create documentation:
    Write a README.md file explaining how to set up and run the application, its architecture, and any design decisions made.

This implementation provides a solid foundation for the news categorization application. It fetches articles from RSS feeds, processes them asynchronously using Celery, categorizes them using a simple rule-based system, and stores them in a PostgreSQL database.

To export the resulting data, you can add a function to dump the database contents to a file:

```python
# In main.py, add this function:

import json
from sqlalchemy.orm import class_mapper

def export_data(format='json'):
    session = Session()
    try:
        articles = session.query(Article).all()
        
        if format == 'json':
            def serialize(model):
                columns = [c.key for c in class_mapper(model.__class__).columns]
                return {c: getattr(model, c) for c in columns}
            
            data = [serialize(article) for article in articles]
            with open('articles_export.json', 'w') as f:
                json.dump(data, f, default=str)
        elif format == 'csv':
            import csv
            with open('articles_export.csv', 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['id', 'title', 'content', 'pub_date', 'source_url', 'category'])
                for article in articles:
                    writer.writerow([article.id, article.title, article.content, article.pub_date, article.source_url, article.category])
        else:
            raise ValueError("Unsupported export format")
        
        print(f"Data exported to articles_export.{format}")
    finally:
        session.close()

# Usage:
# export_data('json')  # or export_data('csv')

```

You can call this function to export the data whenever needed.

Would you like me to explain any part of the code in more detail or help with any specific aspect of the implementation?